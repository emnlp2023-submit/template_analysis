{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wandb.apis.public import Api\n",
    "\n",
    "WANDB_KEY = \"\"  # YOUR WANDB KEY\n",
    "PROJECT = \"\"  # YOUR PROJECT\n",
    "\n",
    "wandb_api = Api(api_key=WANDB_KEY)\n",
    "project = wandb_api.runs(PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "all_runs = []\n",
    "all_runs_paths = set()\n",
    "for run in tqdm(project):\n",
    "    if run.state == 'finished':\n",
    "        x_path = os.path.join(*run.path)\n",
    "        if x_path in all_runs_paths:\n",
    "            continue\n",
    "        else:\n",
    "            config = run.config\n",
    "            history = run.history()\n",
    "            if len(history) > 0:\n",
    "                scores = history.scores.item()\n",
    "                dataset = config['dataset']\n",
    "                model = config['model']\n",
    "                prediction_method = config['prediction_method']\n",
    "\n",
    "                all_runs.append({\"dataset\": dataset, \"model\": model, 'seed': config['seed'],\n",
    "                                 \"selection_method\": config['example_selection_method'],\n",
    "                                 \"prediction_method\": '_'.join(prediction_method.split(\"_\")[-2:]),\n",
    "                                 \"n_shots\": config['n_shots'], 'scores': scores, 'worst_case': min(scores),\n",
    "                                'mean': np.mean(scores), 'std': np.std(scores)})\n",
    "                all_runs_paths.add(x_path)\n",
    "all_runs_df = pd.DataFrame(all_runs)\n",
    "all_runs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(all_runs_paths, 'all_runs_paths.pt')\n",
    "all_runs_df.to_csv('all_runs_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "dataset_templates = defaultdict(dict)\n",
    "for dataset in ['sst2', 'dbpedia', 'agnews', 'trec']:\n",
    "    for seed in [59, 13, 21]:\n",
    "        for run in project:\n",
    "            if run.state == 'finished' and run.config.get('dataset', 's') == dataset and run.config.get('seed', 0) == seed:\n",
    "                templates = run.history().templates.item()\n",
    "                dataset_templates[dataset][seed] = templates\n",
    "                print(templates)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_template_tables[dataset].sort_values(list(table.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_template_tables = {}\n",
    "\n",
    "all_templates = []\n",
    "for dataset in datasets:\n",
    "    table = {'inp_verbalizer': [], 'out_verbalizer': [], 'sep': [], 'big_sep': [], 'seed': [], 'n': []}\n",
    "    for seed in dataset_templates[dataset]:\n",
    "        for i, template in enumerate(dataset_templates[dataset][seed]):\n",
    "            table['inp_verbalizer'].append(template[0])\n",
    "            table['out_verbalizer'].append(template[1])\n",
    "            table['sep'].append(f\"{repr(template[2])}\")\n",
    "            table['big_sep'].append(f\"{repr(template[3])}\")\n",
    "            table['seed'].append(seed)\n",
    "            table['n'].append(i)\n",
    "    dataset_template_tables[dataset] = pd.DataFrame(table)\n",
    "    print(dataset)\n",
    "    display(dataset_template_tables[dataset].sort_values(list(table.keys())[:-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = dataset_template_tables['dbpedia'].sort_values(list(table.keys())[:-2])\n",
    "row = table.iloc[0]\n",
    "next_row = table.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ['inp_verbalizer', 'out_verbalizer', 'sep']:\n",
    "    print(row[k] == next_row[k], k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_good_pairs = {}\n",
    "for dataset in datasets:\n",
    "    good_pairs = []\n",
    "    table = dataset_template_tables[dataset].sort_values(list(table.keys())[:-2])\n",
    "    for i in range(len(table) - 1):\n",
    "        row = table.iloc[i]\n",
    "        next_row = table.iloc[i + 1]\n",
    "        if all([row[k] == next_row[k] for k in ['inp_verbalizer', 'out_verbalizer', 'sep']]) and row['big_sep'] != next_row['big_sep']:\n",
    "            good_pairs.append(((row.seed, row.n), (next_row.seed, next_row.n)))\n",
    "            \n",
    "    dataset_good_pairs[dataset] = good_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.stats import pearsonr\n",
    "a = [template_a_scores[k] for k in template_a_scores]\n",
    "b = [template_b_scores[k] for k in template_a_scores]\n",
    "pearsonr(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    selected_runs = all_runs_df.loc[all_runs_df['dataset'] == dataset]\n",
    "    for pair in dataset_good_pairs[dataset]:\n",
    "        template_a, template_b = pair\n",
    "        template_a_scores, template_b_scores = [], []\n",
    "        for model in names_to_checkpoints:\n",
    "            for prediction_method in ['direct_False', 'channel_True', 'calibrate_True']:\n",
    "                for selection_method in ['random', 'implicitly_topic_models', 'z-ICL']:\n",
    "                    for n_shots in [2, 4, 8]:\n",
    "                        pair_runs = selected_runs.loc[selected_runs['n_shots'] == n_shots]\n",
    "                        pair_runs = pair_runs.loc[selected_runs['selection_method'] == selection_method]\n",
    "                        pair_runs = pair_runs.loc[selected_runs['prediction_method'] == prediction_method]\n",
    "                        pair_runs = pair_runs.loc[selected_runs['model'] == model]\n",
    "                        a_run = pair_runs.loc[pair_runs['seed'] == template_a[0]]\n",
    "                        b_run = pair_runs.loc[pair_runs['seed'] == template_b[0]]\n",
    "                        if len(a_run) > 0 and len(b_run) > 0:\n",
    "                            template_a_scores.append(a_run['scores'].values[0][template_a[1]])\n",
    "                            template_b_scores.append(b_run['scores'].values[0][template_b[1]])\n",
    "                            \n",
    "        corr, p_value = pearsonr(template_a_scores, template_b_scores)\n",
    "        print(f\"{template_a} correlation with {template_b}: {corr:.3f} with p_value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Baseline results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "n_shots_tables = {}\n",
    "selected_runs = all_runs_df.loc[all_runs_df['selection_method'] == 'random']\n",
    "selected_runs = selected_runs.loc[selected_runs['prediction_method'] == 'direct_False']\n",
    "for n_shots in [2, 4, 8]:\n",
    "    n_shots_runs = selected_runs.loc[selected_runs['n_shots'] == n_shots]\n",
    "    table = {dataset: defaultdict(str) for dataset in datasets}\n",
    "    for model in names_to_checkpoints:\n",
    "        model_runs = n_shots_runs.loc[n_shots_runs['model'] == model]\n",
    "        for dataset in datasets:\n",
    "            dataset_runs = model_runs.loc[model_runs['dataset'] == dataset]\n",
    "            scores = []\n",
    "            for seed in [59, 13, 21]:\n",
    "                seed_scores = dataset_runs.loc[dataset_runs['seed'] == seed]['scores']\n",
    "                if len(seed_scores) > 0:\n",
    "                    seed_scores = seed_scores.values[0]\n",
    "                scores.extend(seed_scores)\n",
    "            table[dataset][model] = f\"{np.mean(scores):.3f} ± {np.std(scores):.3f}\"\n",
    "    table = pd.DataFrame(table, index=names_to_checkpoints.keys(), columns=datasets)\n",
    "    n_shots_tables[n_shots] = table\n",
    "    print(f\"{n_shots}-shot\")\n",
    "    display(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in ['gpt2-large', 'gpt2-xl',\n",
    "              'llama-7b', 'llama-13b', 'llama-30b', 'llama-65b',\n",
    "              ]:\n",
    "    print(model, end=' & ')\n",
    "    for dataset in datasets:\n",
    "        for n_shots in [2, 4, 8]:\n",
    "            res = n_shots_tables[n_shots].loc[model, dataset]\n",
    "            mean, std = map(float, res.split(\" ± \"))\n",
    "            end = '\\\\\\\\\\n' if (n_shots == 8) and (dataset == 'trec') else ' & '\n",
    "            print(f\"{mean:.2f}\\\\textsubscript{{{std:.2f}}}\", end=end)\n",
    "    if model == 'gpt2-xl':\n",
    "        print(\"\\\\midrule\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[16, 12])\n",
    "for i, dataset in enumerate(datasets, 1):\n",
    "    plt.subplot(2, 2, i)\n",
    "    for n_shots in [2, 4, 8]:\n",
    "        table = n_shots_tables[n_shots]\n",
    "        x, y, y_plus, y_minus = [], [], [], []\n",
    "        for model in ['llama-7b', 'llama-13b', 'llama-30b', 'llama-65b',\n",
    "                      'falcon-1b', 'falcon-7b', 'falcon-40b']:\n",
    "            x.append(model)\n",
    "            res = table.loc[model, dataset]\n",
    "            mean, std = [float(x) for x in res.split(' ± ')]\n",
    "            y.append(mean)\n",
    "            y_plus.append(mean + std)\n",
    "            y_minus.append(mean - std)\n",
    "        plt.plot(x, y, label=f\"{n_shots}-shot\")\n",
    "        plt.fill_between(x, y_minus, y_plus, alpha=0.1)\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2. Prediction methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_runs = all_runs_df.loc[all_runs_df['n_shots'] == int(n_shots.split('-')[0])]\n",
    "selected_runs = selected_runs.loc[selected_runs['dataset'] == dataset]\n",
    "selection_method = 'random' if n_shots == '2-shot' else '0-shot'\n",
    "selected_runs = selected_runs.loc[selected_runs['selection_method'] == selection_method]\n",
    "selected_runs.loc[selected_runs['prediction_method'] == 'direct_False']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_shots = '0-shot'\n",
    "dataset = 'sst2'\n",
    "selected_runs = all_runs_df.loc[all_runs_df['n_shots'] == int(n_shots.split('-')[0])]\n",
    "selected_runs = selected_runs.loc[selected_runs['dataset'] == dataset]\n",
    "selection_method = 'random' if n_shots == '2-shot' else '0-shot'\n",
    "selected_runs = selected_runs.loc[selected_runs['selection_method'] == selection_method]\n",
    "method = 'direct_False'\n",
    "method_runs = selected_runs.loc[selected_runs['prediction_method'] == method]\n",
    "method_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = ['direct_False', 'channel_True',  'calibrate_True',]\n",
    "\n",
    "def combine_scores(runs, aggregation_method='all', prediction_method='direct_False'):\n",
    "    all_scores = []\n",
    "    for _, run in runs.iterrows():\n",
    "        if aggregation_method == 'direct_best':\n",
    "            if method in ['calibrate_True', 'channel_True']:\n",
    "                all_scores.extend(run['scores'])\n",
    "            else:\n",
    "                all_scores.append(max(run['scores']))\n",
    "        elif aggregation_method == 'worst':\n",
    "            all_scores.append(min(run['scores']))\n",
    "        elif aggregation_method == 'all':\n",
    "            all_scores.extend(run['scores'])\n",
    "            \n",
    "    return all_scores\n",
    "\n",
    "method_name = {'direct_False': 'Direct', 'channel_True': 'Channel', 'calibrate_True': 'Calibrate'}\n",
    "\n",
    "def plot(n_shots_range, aggregation_method='all'):\n",
    "    res = []\n",
    "    for n_shots in n_shots_range:\n",
    "        fig = plt.figure(figsize=[24, 6])\n",
    "        for i, dataset in enumerate(datasets, 1):\n",
    "            ax = fig.add_subplot(1, 4, i)\n",
    "            \n",
    "            selected_runs = all_runs_df.loc[all_runs_df['n_shots'] == int(n_shots.split('-')[0])]\n",
    "            selected_runs = selected_runs.loc[selected_runs['dataset'] == dataset]\n",
    "            selection_method = 'random' if n_shots == '2-shot' else '0-shot'\n",
    "            selected_runs = selected_runs.loc[selected_runs['selection_method'] == selection_method]\n",
    "            \n",
    "            for method in methods:\n",
    "                method_runs = selected_runs.loc[selected_runs['prediction_method'] == method]\n",
    "                x, mean, std, = [], [], []\n",
    "                for model in names_to_checkpoints:\n",
    "                    x.append(model)\n",
    "                    model_runs = method_runs.loc[method_runs['model'] == model]\n",
    "                    model_scores = combine_scores(model_runs, prediction_method=method, \n",
    "                                                  aggregation_method=aggregation_method)\n",
    "                    mean.append(np.mean(model_scores))\n",
    "                    std.append(np.std(model_scores))\n",
    "                ax.plot(x, mean, label=method_name[method])\n",
    "                mean = np.array(mean)\n",
    "                std = np.array(std)\n",
    "                ax.fill_between(x, mean - std, mean + std, alpha=0.1)\n",
    "            if i == 1:\n",
    "                ax.legend(loc='lower right')\n",
    "            ax.set_xticks(ticks=list(range(len(names_to_checkpoints))),\n",
    "                       labels=names_to_checkpoints.keys(),\n",
    "                       rotation=45, ha=\"right\")\n",
    "            ax.set_title(f\"{n_shots} accuracy on {dataset_name[dataset]}\")\n",
    "            ax.plot()\n",
    "        res.append(fig)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "matplotlib.rcParams.update({\n",
    "        \"font.family\": \"Times New Roman\",\n",
    "        \"axes.labelsize\": 18,\n",
    "        \"font.size\": 20,\n",
    "        \"legend.fontsize\": 16,\n",
    "        \"xtick.labelsize\": 13,\n",
    "        \"ytick.labelsize\": 14,\n",
    "        \"text.usetex\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot(['2-shot'])[0]\n",
    "fig.savefig('figs/prediction_methods_main.pdf', format='pdf', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs = plot(['0-shot', '2-shot'], aggregation_method='direct_best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs = plot(['0-shot', '2-shot'], aggregation_method='worst')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_runs_df = pd.read_csv(\"all_runs_df.csv\")\n",
    "all_runs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_runs_df[\"scores\"] = all_runs_df[\"scores\"].apply(lambda x: eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "names_to_checkpoints = {'gpt2-large': 'gpt2-large',\n",
    "                        'gpt2-xl': 'gpt2-xl',\n",
    "                        'gptj': 'EleutherAI/gpt-j-6B',\n",
    "                        'gpt-neox': 'EleutherAI/gpt-neox-20b',\n",
    "                        'opt-1.3b': 'facebook/opt-1.3b',\n",
    "                        'opt-6.7b': \"facebook/opt-6.7b\",\n",
    "                        'opt-30b': \"facebook/opt-30b\",\n",
    "                        'opt-66b': \"facebook/opt-66b\",\n",
    "                        'bloom-1.7b': 'bigscience/bloom-1b7',\n",
    "                        'bloom-3b': 'bigscience/bloom-3b',\n",
    "                        'bloom-7.1b': 'bigscience/bloom-7b1',\n",
    "                        'pythia-6.9b': 'EleutherAI/pythia-6.9b',\n",
    "                        'pythia-12b': 'EleutherAI/pythia-12b',\n",
    "                        'cerebras-6.7b': 'cerebras/Cerebras-GPT-6.7B',\n",
    "                        'cerebras-13b': 'cerebras/Cerebras-GPT-13B',\n",
    "                        'llama-7b': 'Neko-Institute-of-Science/LLaMA-7B-HF',\n",
    "                        'llama-13b': 'Neko-Institute-of-Science/LLaMA-13B-HF',\n",
    "                        'llama-30b': 'Neko-Institute-of-Science/LLaMA-30B-hf',\n",
    "                        'llama-65b': 'Neko-Institute-of-Science/LLaMA-65B-hf',\n",
    "                        'falcon-1b': 'tiiuae/falcon-rw-1b',\n",
    "                        'falcon-7b': 'tiiuae/falcon-7b',\n",
    "                        'falcon-40b': 'tiiuae/falcon-40b',\n",
    "}\n",
    "\n",
    "prediction_methods = ['direct_False', \n",
    "                      'channel_True', \n",
    "                      'calibrate_True',]\n",
    "\n",
    "def aggregate_scores(df, seeds, conditions, method='str'):\n",
    "    selected_runs = df\n",
    "    for condition in conditions:\n",
    "        k, v = list(condition.items())[0]\n",
    "        selected_runs = selected_runs[selected_runs[k] == v]\n",
    "    scores = []\n",
    "    for seed in seeds:\n",
    "        run = selected_runs[selected_runs['seed'] == seed]\n",
    "        if len(run) < 1:\n",
    "            print(conditions, seed)\n",
    "        if len(run) != 0:\n",
    "            scores.extend(run['scores'].values[0])\n",
    "    if len(scores) == 0:\n",
    "        out = 'NaN'\n",
    "    else:\n",
    "        if method == 'str':\n",
    "            out = f\"{np.mean(scores):.3f} ± {np.std(scores):.2f}\"\n",
    "            if len(scores) != len(seeds) * 10:\n",
    "                out += f\" ({len(scores)})\"\n",
    "        elif method == 'mean':\n",
    "            out = round(np.mean(scores), 3)\n",
    "        elif method == 'worst':\n",
    "            out = min(scores)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "datasets = ['sst2', 'dbpedia', 'agnews', 'trec']\n",
    "tables1 = {'0-shot': {}, '2-shot': {}}\n",
    "for n_shots in [0, 2]:\n",
    "    conditions = []\n",
    "    \n",
    "    seeds = [59] if n_shots == 0 else [59, 13, 21]\n",
    "    selection_method = '0-shot' if n_shots == 0 else 'random'\n",
    "    \n",
    "    conditions.append({'selection_method': selection_method})\n",
    "    conditions.append({'n_shots': n_shots})\n",
    "    for dataset in datasets:\n",
    "        conditions.append({'dataset': dataset})\n",
    "        table = []\n",
    "        for model in names_to_checkpoints:\n",
    "            if model == 'opt-30b':\n",
    "                seeds = [13]\n",
    "            else:\n",
    "                seeds = [59] if n_shots == 0 else [59, 13, 21]\n",
    "            conditions.append({'model': model})\n",
    "            entry = {'model': model}\n",
    "            for method in prediction_methods:\n",
    "                conditions.append({'prediction_method': method})\n",
    "                res = aggregate_scores(all_runs_df, seeds, conditions)\n",
    "                # res = f\"mean ± std\"\n",
    "                entry.update({method: res})\n",
    "                conditions.remove({'prediction_method': method})\n",
    "            conditions.remove({'model': model})\n",
    "            table.append(entry)\n",
    "        table = pd.DataFrame(table)\n",
    "        table.set_index('model', inplace=True)\n",
    "        tables1[f\"{n_shots}-shot\"][dataset] = table\n",
    "        conditions.remove({'dataset': dataset})\n",
    "        \n",
    "\n",
    "for n_shots in [0, 2]:\n",
    "    print(f'{n_shots}-shot')\n",
    "    for d in datasets:\n",
    "        print(d)\n",
    "        display(tables1[f'{n_shots}-shot'][d])\n",
    "    print(\"-------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from collections import defaultdict\n",
    "\n",
    "from utils import load_split_dataset, names_to_checkpoints\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "mega_mega_res_df = {}\n",
    "for dataset in [\"sst2\", \"agnews\", \"dbpedia\", \"trec\"]:\n",
    "    base_path = f\"templates_ensemble_2-shot_{dataset}/\"\n",
    "    train, val, labels_mp = load_split_dataset(dataset, seed=59)\n",
    "\n",
    "    mega_res_df = {}\n",
    "\n",
    "    for name in names_to_checkpoints:\n",
    "        if not os.path.exists(base_path + f\"{name}_direct_13\"):\n",
    "            continue\n",
    "        mega_res_df[name] = {}\n",
    "        mode_means = defaultdict(list)\n",
    "        mean_means = defaultdict(list)\n",
    "        seeds = [13, 21, 59]\n",
    "        for seed in seeds:\n",
    "            res = torch.load(base_path + f\"{name}_direct_{seed}\")\n",
    "            for size in range(1, 11):\n",
    "                mode = stats.mode(np.array(res['results'][:size])).mode[0]\n",
    "                mode_mean = (mode == val['target']).mean()\n",
    "                probs = torch.stack([\n",
    "                    res['probs'][i] for i in range(len(res['probs'][:size]))\n",
    "                ])\n",
    "                answers = [labels_mp[x.item()] for x in probs.mean(dim=0).argmax(dim=1)]\n",
    "                mean_mean = (answers == val['target']).mean()\n",
    "\n",
    "                mode_means[size].append(mode_mean)\n",
    "                mean_means[size].append(mean_mean)\n",
    "\n",
    "        stds = []\n",
    "        for size in range(1, 11):\n",
    "            mega_res_df[name][size] = f\"{np.mean(mean_means[size]):.3f} ± {np.std(mean_means[size]):.2f}\"\n",
    "            stds.append(np.std(mean_means[size]))\n",
    "\n",
    "    mega_mega_res_df[dataset] = mega_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in mega_mega_res_df:\n",
    "    ensemble_10, ensemble_5, ensemble_3 = [], [], []\n",
    "    for model in list(tables1[\"2-shot\"][dataset].index):\n",
    "        if model in mega_mega_res_df[dataset]:\n",
    "            ensemble_10.append(mega_mega_res_df[dataset][model][10])\n",
    "            ensemble_3.append(mega_mega_res_df[dataset][model][3])\n",
    "            ensemble_5.append(mega_mega_res_df[dataset][model][5])\n",
    "        else:\n",
    "            ensemble_10.append(\"???\")\n",
    "            ensemble_3.append(\"???\")\n",
    "            ensemble_5.append(\"???\")\n",
    "    tables1[\"2-shot\"][dataset][\"3_direct\"] = ensemble_3\n",
    "    tables1[\"2-shot\"][dataset][\"5_direct\"] = ensemble_5\n",
    "    tables1[\"2-shot\"][dataset][\"10_direct\"] = ensemble_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in tables1['2-shot']:\n",
    "    print(d)\n",
    "    display(tables1['2-shot'][d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mega_mega_res_df = {}\n",
    "for dataset in [\"sst2\", \"agnews\", \"dbpedia\", \"trec\"]:\n",
    "    base_path = f\"ensembles_channel_true/{dataset}/2_shot/\"\n",
    "    train, val, labels_mp = load_split_dataset(dataset, seed=59)\n",
    "\n",
    "    mega_res_df = {}\n",
    "\n",
    "    for name in names_to_checkpoints:\n",
    "        if not os.path.exists(base_path + f\"{name}_channel_13\"):\n",
    "            continue\n",
    "        mega_res_df[name] = {}\n",
    "        mode_means = defaultdict(list)\n",
    "        mean_means = defaultdict(list)\n",
    "        seeds = [13, 21, 59]\n",
    "        for seed in seeds:\n",
    "            res = torch.load(base_path + f\"{name}_channel_{seed}\")\n",
    "            for size in range(1, 11):\n",
    "                mode = stats.mode(np.array(res['results'][:size])).mode[0]\n",
    "                mode_mean = (mode == val['target']).mean()\n",
    "                probs = torch.stack([\n",
    "                    res['probs'][i] for i in range(len(res['probs'][:size]))\n",
    "                ])\n",
    "                answers = [labels_mp[x.item()] for x in probs.mean(dim=0).argmax(dim=1)]\n",
    "                mean_mean = (answers == val['target']).mean()\n",
    "\n",
    "                mode_means[size].append(mode_mean)\n",
    "                mean_means[size].append(mean_mean)\n",
    "\n",
    "        stds = []\n",
    "        for size in range(1, 11):\n",
    "            mega_res_df[name][size] = f\"{np.mean(mean_means[size]):.3f} ± {np.std(mean_means[size]):.2f}\"\n",
    "            stds.append(np.std(mean_means[size]))\n",
    "\n",
    "    mega_mega_res_df[dataset] = mega_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in mega_mega_res_df:\n",
    "    ensemble_10, ensemble_5, ensemble_3 = [], [], []\n",
    "    for model in list(tables1[\"2-shot\"][dataset].index):\n",
    "        if model in mega_mega_res_df[dataset]:\n",
    "            ensemble_10.append(mega_mega_res_df[dataset][model][10])\n",
    "            ensemble_3.append(mega_mega_res_df[dataset][model][3])\n",
    "            ensemble_5.append(mega_mega_res_df[dataset][model][5])\n",
    "        else:\n",
    "            ensemble_10.append(\"???\")\n",
    "            ensemble_3.append(\"???\")\n",
    "            ensemble_5.append(\"???\")\n",
    "    tables1[\"2-shot\"][dataset][\"3_channel\"] = ensemble_3\n",
    "    tables1[\"2-shot\"][dataset][\"5_channel\"] = ensemble_5\n",
    "    tables1[\"2-shot\"][dataset][\"10_channel\"] = ensemble_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in tables1['2-shot']:\n",
    "    print(d)\n",
    "    display(tables1['2-shot'][d][tables1['2-shot'][d]['5_channel'] != \"???\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "mega_mega_res_df = {}\n",
    "for dataset in [\"sst2\", \"agnews\", \"dbpedia\", \"trec\"]:\n",
    "    base_path = f\"ensembles_calibrate_true/{dataset}/2_shot/\"\n",
    "    train, val, labels_mp = load_split_dataset(dataset, seed=59)\n",
    "\n",
    "    mega_res_df = {}\n",
    "\n",
    "    for name in names_to_checkpoints:\n",
    "        if not os.path.exists(base_path + f\"{name}_calibrate_13\"):\n",
    "            continue\n",
    "        mega_res_df[name] = {}\n",
    "        mode_means = defaultdict(list)\n",
    "        mean_means = defaultdict(list)\n",
    "        seeds = [13, 21, 59] #+ list(range(0, 7))\n",
    "        for seed in seeds:\n",
    "            res = torch.load(base_path + f\"{name}_calibrate_{seed}\")\n",
    "            for size in range(1, 11):\n",
    "                mode = stats.mode(np.array(res['results'][:size])).mode[0]\n",
    "                mode_mean = (mode == val['target']).mean()\n",
    "                probs = torch.stack([\n",
    "                    res['probs'][i] for i in range(len(res['probs'][:size]))\n",
    "                ])\n",
    "                answers = [labels_mp[x.item()] for x in probs.mean(dim=0).argmax(dim=1)]\n",
    "                mean_mean = (answers == val['target']).mean()\n",
    "\n",
    "                mode_means[size].append(mode_mean)\n",
    "                mean_means[size].append(mean_mean)\n",
    "\n",
    "        stds = []\n",
    "        for size in range(1, 11):\n",
    "            mega_res_df[name][size] = f\"{np.mean(mean_means[size]):.3f} ± {np.std(mean_means[size]):.2f}\"\n",
    "            stds.append(np.std(mean_means[size]))\n",
    "\n",
    "    mega_mega_res_df[dataset] = mega_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in mega_mega_res_df:\n",
    "    ensemble_10, ensemble_5, ensemble_3 = [], [], []\n",
    "    for model in list(tables1[\"2-shot\"][dataset].index):\n",
    "        if model in mega_mega_res_df[dataset]:\n",
    "            ensemble_10.append(mega_mega_res_df[dataset][model][10])\n",
    "            ensemble_3.append(mega_mega_res_df[dataset][model][3])\n",
    "            ensemble_5.append(mega_mega_res_df[dataset][model][5])\n",
    "        else:\n",
    "            ensemble_10.append(\"???\")\n",
    "            ensemble_3.append(\"???\")\n",
    "            ensemble_5.append(\"???\")\n",
    "    tables1[\"2-shot\"][dataset][\"3_calibrate\"] = ensemble_3\n",
    "    tables1[\"2-shot\"][dataset][\"5_calibrate\"] = ensemble_5\n",
    "    tables1[\"2-shot\"][dataset][\"10_calibrate\"] = ensemble_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in tables1['2-shot']:\n",
    "    print(d)\n",
    "    display(\n",
    "        tables1['2-shot'][d][tables1['2-shot'][d]['5_calibrate'] != \"???\"][[\"direct_False\", \"5_direct\", \"channel_True\", \"5_channel\", \"calibrate_True\", \"5_calibrate\"]]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_order = [\"calibrate_True\"] +\\\n",
    "            [f\"{x}_calibrate\" for x in [3, 5, 10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_df = tables1['2-shot'][\"sst2\"][tables1['2-shot'][\"sst2\"]['5_calibrate'] != \"???\"]\n",
    "print(\n",
    "    latex_df[col_order].to_latex()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "nice_fonts = {\n",
    "        # Use LaTeX to write all text\n",
    "        \"text.usetex\": True,\n",
    "        \"font.family\": \"Times New Roman\",\n",
    "        # Use 10pt font in plots, to match 10pt font in document\n",
    "        \"axes.labelsize\": 14,\n",
    "        \"font.size\": 14,\n",
    "        # Make the legend/label fonts a little smaller\n",
    "        \"legend.fontsize\": 14,\n",
    "        \"xtick.labelsize\": 14,\n",
    "        \"ytick.labelsize\": 14,\n",
    "}\n",
    "matplotlib.rcParams.update(nice_fonts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from collections import defaultdict\n",
    "\n",
    "from utils import load_split_dataset, names_to_checkpoints\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "dataset = \"sst2\"\n",
    "base_path = {\n",
    "    \"direct\": f\"templates_ensemble_2-shot_{dataset}/\",\n",
    "    \"calibrate\": f\"ensembles_calibrate_true/{dataset}/2_shot/\",\n",
    "    \"channel\": f\"ensembles_channel_true/{dataset}/2_shot/\",\n",
    "}\n",
    "train, val, labels_mp = load_split_dataset(dataset, seed=59)\n",
    "\n",
    "mega_res_df = {}\n",
    "stds = {}\n",
    "\n",
    "for method in [\n",
    "    \"direct\",\n",
    "    \"calibrate\",\n",
    "    \"channel\"\n",
    "]:\n",
    "    mega_res_df[method] = {}\n",
    "    stds[method] = {}\n",
    "    for name in [\"gpt2-large\", \"gpt2-xl\", \"llama-7b\", \"llama-13b\", \"llama-30b\", \"llama-65b\"]:\n",
    "        stds[method][name] = []\n",
    "        mega_res_df[method][name] = {}\n",
    "        mode_means = defaultdict(list)\n",
    "        mean_means = defaultdict(list)\n",
    "        seeds = [13, 21, 59]\n",
    "        for seed in seeds:\n",
    "            res = torch.load(base_path[method] + f\"{name}_{method}_{seed}\")\n",
    "            for size in range(1, 11):\n",
    "                mode = stats.mode(np.array(res['results'][:size])).mode[0]\n",
    "                mode_mean = (mode == val['target']).mean()\n",
    "                probs = torch.stack([\n",
    "                    res['probs'][i] for i in range(len(res['probs'][:size]))\n",
    "                ])\n",
    "                answers = [labels_mp[x.item()] for x in probs.mean(dim=0).argmax(dim=1)]\n",
    "                mean_mean = (answers == val['target']).mean()\n",
    "\n",
    "                mode_means[size].append(mode_mean)\n",
    "                mean_means[size].append(mean_mean)\n",
    "\n",
    "        for size in range(1, 11):\n",
    "            mega_res_df[method][name][size] = np.mean(mean_means[size])\n",
    "            stds[method][name].append(np.std(mean_means[size]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = range(1, 11)\n",
    "\n",
    "fig, axs = plt.subplots(3, 2, figsize=(6.4 * 2, 4 * 3))\n",
    "fig.tight_layout() \n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "for name in mega_res_df[\"channel\"]:\n",
    "    direct = float(tables1[\"2-shot\"][dataset][\"direct_False\"][name][:5])\n",
    "    calibrate = float(tables1[\"2-shot\"][dataset][\"calibrate_True\"][name][:5])\n",
    "    channel = float(tables1[\"2-shot\"][dataset][\"channel_True\"][name][:5])\n",
    "\n",
    "    for method in [\n",
    "        \"direct\",\n",
    "        \"calibrate\",\n",
    "        \"channel\"\n",
    "    ]:\n",
    "        values = list(mega_res_df[method][name].values())\n",
    "        axs[i, j].plot(x, values, label=f\"Channel + Ensemble\")\n",
    "        axs[i, j].fill_between(range(1, 11),\n",
    "                     np.array(values) + np.array(stds[method][name]),\n",
    "                     np.array(values) - np.array(stds[method][name]),\n",
    "                     alpha=0.2)\n",
    "\n",
    "    axs[i, j].plot(x, [direct] * 10, label=\"Direct\", linestyle=\"dashed\")\n",
    "    axs[i, j].plot(x, [calibrate] * 10, label=\"Calibrate\", linestyle=\"dashed\")\n",
    "    axs[i, j].plot(x, [channel] * 10, label=\"Channel\", linestyle=\"dashed\", color=\"tab:blue\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    axs[i, j].set_title(f\"{spelling[name]}\")\n",
    "    axs[i, j].set_xlabel(\"Ensemble size\")\n",
    "    axs[i, j].set_ylabel(\"Accuracy\")\n",
    "    axs[i, j].legend(loc=\"lower right\")\n",
    "    \n",
    "    j += 1\n",
    "    if j == 2:\n",
    "        j = 0\n",
    "        i += 1\n",
    "\n",
    "plt.subplots_adjust(hspace=0.3)\n",
    "plt.savefig(f'../pictures/all_ensemble_2shot.pdf', format='pdf', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0-shot to 2-shot transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lena_templates = {}\n",
    "\n",
    "for dataset in dataset_templates:\n",
    "    lena_templates[dataset] = {}\n",
    "    for seed in dataset_templates[dataset]:\n",
    "        lena_templates[dataset][seed] = [''.join(t[:-1]) for t in dataset_templates[dataset][seed]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_runs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_t_to_s(df):\n",
    "    templates = lena_templates[df[\"dataset\"]][df[\"seed\"]]\n",
    "    return {t: s for t, s in zip(templates, df[\"scores\"])}\n",
    "\n",
    "all_runs_df[\"templates_to_scores\"] = all_runs_df.apply(make_t_to_s, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_runs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bests = []\n",
    "others = []\n",
    "for seed in [13, 21, 59]:\n",
    "    s = few_shot[(few_shot[\"n_shots\"] == 2) & (few_shot[\"seed\"] == seed)][\"templates_to_scores\"].values[0]\n",
    "    s = {k: v for k, v\n",
    "             in sorted(s.items(),\n",
    "                       key=lambda x: x[1],\n",
    "                       reverse=True)}\n",
    "    bests.append(list(s.values())[0])\n",
    "    others += list(s.values())\n",
    "    print(s)\n",
    "    print()\n",
    "print(bests)\n",
    "print(others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"sst2\"\n",
    "method = \"direct_False\"\n",
    "\n",
    "few_shot_res = {}\n",
    "for model in [\"gpt2-large\", \"gpt2-xl\", \"llama-7b\", \"llama-13b\", \"llama-30b\", \"llama-65b\"]:#names_to_checkpoints:\n",
    "    try:\n",
    "        zero_shot = torch.load(f\"template_selection/{dataset}/{model}_formats_stats_zero_shot_{method}\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "        continue\n",
    "    \n",
    "    few_shot = all_runs_df[\n",
    "        (all_runs_df[\"prediction_method\"] == method) &\n",
    "        (all_runs_df[\"model\"] == model) &\n",
    "        (all_runs_df[\"dataset\"] == dataset) &\n",
    "        (all_runs_df[\"selection_method\"] == \"random\")\n",
    "    ].sort_values(by=[\"n_shots\", \"seed\"])\n",
    "    \n",
    "    few_shot_res[model] = {}\n",
    "    for n_shot in [2, 4, 8]:\n",
    "        few_shot_res[model][n_shot] = {}\n",
    "        bests = []\n",
    "        others = []\n",
    "        zero_on_few = []\n",
    "        for seed in [13, 21, 59]:\n",
    "            s = few_shot[(few_shot[\"n_shots\"] == n_shot) & (few_shot[\"seed\"] == seed)][\"templates_to_scores\"].values[0]\n",
    "            s = {k: v for k, v\n",
    "                     in sorted(s.items(),\n",
    "                               key=lambda x: x[1],\n",
    "                               reverse=True)}\n",
    "            bests.append(list(s.values())[0])\n",
    "            others += list(s.values())[1:]\n",
    "            \n",
    "            if n_shot !=8:\n",
    "                zero_on_few.append(\n",
    "                    torch.load(f\"gpt2_llama_fewshot/{model}_{dataset}_{method}_{n_shot}shot_{seed}\")[\"\\n\"]\n",
    "                )\n",
    "            \n",
    "        few_shot_res[model][n_shot][\"best\"] = (np.mean(bests), np.std(bests))\n",
    "        few_shot_res[model][n_shot][\"others\"] = (np.mean(others), np.std(others))\n",
    "        few_shot_res[model][n_shot][\"zero_on_few\"] = (np.mean(zero_on_few), np.std(zero_on_few))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in few_shot_res:\n",
    "    for n_shot in few_shot_res[model]:\n",
    "        for k in few_shot_res[model][n_shot]:\n",
    "            mean = few_shot_res[model][n_shot][k][0]\n",
    "            std = few_shot_res[model][n_shot][k][1]\n",
    "            few_shot_res[model][n_shot][k] = f\"{mean:.2f}_\\textsubscript{std:.2f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in few_shot_res:\n",
    "    print(model)\n",
    "    print(\n",
    "        pd.DataFrame(few_shot_res[model]).transpose()[[\"zero_on_few\", \"best\", \"others\"]].to_latex()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "for model in [\n",
    "    \"bloom-1.7b\", \"bloom-3b\", \"bloom-7.1b\",\n",
    "    \"cerebras-6.7b\", \"cerebras-13b\",\n",
    "    \"gpt2-large\", \"gpt2-xl\", \"gptj\",\n",
    "    \"opt-1.3b\", \"opt-6.7b\",\n",
    "    \"pythia-6.9b\", \"pythia-12b\"\n",
    "]:\n",
    "    zero_shot = torch.load(f\"template_selection/agnews/{model}_formats_stats_zero_shot_direct_False\")\n",
    "    few_shot = torch.load(f\"agnews_random_shot_direct_false/{model}_res/stats\")\n",
    "    zero_shot = {k.replace(\"{}\", \"{}\" + k[-1], 1)[:-1]: v for k, v in zero_shot.items()}\n",
    "    zero_shot = {k: v for k, v\n",
    "                 in sorted(zero_shot.items(),\n",
    "                           key=lambda x: x[1],\n",
    "                           reverse=True)}\n",
    "    \n",
    "    few_shot_res = {}\n",
    "    for n_shot in [2, 4, 8]:\n",
    "        few_shot[n_shot] = defaultdict(list)\n",
    "        for seed in [13, 21, 59]:\n",
    "            for key in few_shot[f\"{n_shot}_shot_{seed}\"]:\n",
    "                few_shot[n_shot][key].append(np.array(few_shot[f\"{n_shot}_shot_{seed}\"][key]))\n",
    "        few_shot_res[f\"{n_shot}_mean\"] = {k: np.mean(v, axis=0) for k, v in few_shot[n_shot].items()}\n",
    "        few_shot_res[f\"{n_shot}_std\"] = {k: np.mean(v, axis=0) for k, v in few_shot[n_shot].items()}\n",
    "        \n",
    "        few_shot_res[f\"{n_shot}_mean\"] = {k: v for k, v\n",
    "                                          in sorted(few_shot_res[f\"{n_shot}_mean\"].items(),\n",
    "                                                    key=lambda x: x[1],\n",
    "                                                    reverse=True)}\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"direct_False\"\n",
    "dataset = \"sst2\"\n",
    "\n",
    "best_templates = {}\n",
    "best_templates[dataset] = {}\n",
    "best_templates[dataset][method] = {}\n",
    "\n",
    "for model in names_to_checkpoints:\n",
    "        try:\n",
    "            zero_shot = torch.load(f\"template_selection/{dataset}/{model}_formats_stats_zero_shot_{method}\")\n",
    "        except FileNotFoundError as e:\n",
    "            print(e)\n",
    "            continue\n",
    "        zero_shot = {k: v for k, v\n",
    "                     in sorted(zero_shot.items(),\n",
    "                               key=lambda x: x[1],\n",
    "                               reverse=True)}\n",
    "        best_template = list(zero_shot.keys())[0]\n",
    "        \n",
    "        template = best_template.split(\" {}\")\n",
    "        if best_template.startswith(\"{}\"):\n",
    "                    template.insert(0, \"{}\")\n",
    "                    template[1] = template[1][2:]\n",
    "        else:\n",
    "            template[0] += \" {}\"\n",
    "        template[1] += \" {}\"\n",
    "        best_templates[dataset][method][model] = template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "methods = ['direct_True', 'direct_False', 'channel_True', 'channel_False', 'calibrate_True', 'calibrate_False']\n",
    "for n_shots in ['0-shot', '2-shot']:\n",
    "    worst_table = {'sst2':[], 'dbpedia':[], 'agnews':[], 'trec':[]}\n",
    "    for dataset in mean_table:\n",
    "        table = tables1[n_shots][dataset]\n",
    "        for method in methods:\n",
    "            mean_table[dataset].append(f\"${table[method].min():.3f}_{{\\\\pm{table[method].std():.3f}}}$\")\n",
    "\n",
    "    print(n_shots)\n",
    "    table = pd.DataFrame(mean_table, index=methods)\n",
    "    display(table)\n",
    "    print(table.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "methods = ['direct_True', 'direct_False', 'channel_True', 'channel_False', 'calibrate_True', 'calibrate_False']\n",
    "for n_shots in ['0-shot', '2-shot']:\n",
    "    mean_table = {'sst2':[], 'dbpedia':[], 'agnews':[], 'trec':[]}\n",
    "    for dataset in mean_table:\n",
    "        table = tables1[n_shots][dataset]\n",
    "        for method in methods:\n",
    "            mean_table[dataset].append(f\"${table[method].mean():.3f}_{{\\\\pm{table[method].std():.3f}}}$\")\n",
    "\n",
    "    print(n_shots)\n",
    "    table = pd.DataFrame(mean_table, index=methods)\n",
    "    display(table)\n",
    "    print(table.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "datasets = ['sst2', 'dbpedia', 'agnews', 'trec']\n",
    "tables1 = {'0-shot': {}, '2-shot': {}}\n",
    "for n_shots in [0, 2]:\n",
    "    conditions = []\n",
    "    \n",
    "    seeds = [59] if n_shots == 0 else [59, 13, 21]\n",
    "    selection_method = '0-shot' if n_shots == 0 else 'random'\n",
    "    \n",
    "    conditions.append({'selection_method': selection_method})\n",
    "    conditions.append({'n_shots': n_shots})\n",
    "    for dataset in datasets:\n",
    "        conditions.append({'dataset': dataset})\n",
    "        table = []\n",
    "        for model in names_to_checkpoints:\n",
    "            if model == 'opt-30b':\n",
    "                seeds = [13]\n",
    "            else:\n",
    "                seeds = [59] if n_shots == 0 else [59, 13, 21]\n",
    "            conditions.append({'model': model})\n",
    "            entry = {'model': model}\n",
    "            for method in prediction_methods:\n",
    "                conditions.append({'prediction_method': method})\n",
    "                res = aggregate_scores(all_runs_df, seeds, conditions, method='worst')\n",
    "                entry.update({method: res})\n",
    "                conditions.remove({'prediction_method': method})\n",
    "            conditions.remove({'model': model})\n",
    "            table.append(entry)\n",
    "        table = pd.DataFrame(table)\n",
    "        table.set_index('model', inplace=True)\n",
    "        tables1[f\"{n_shots}-shot\"][dataset] = table\n",
    "        conditions.remove({'dataset': dataset})\n",
    "        \n",
    "methods = ['direct_True', 'direct_False', 'channel_True', 'channel_False', 'calibrate_True', 'calibrate_False']\n",
    "for n_shots in ['0-shot', '2-shot']:\n",
    "    worst_table = {'sst2':[], 'dbpedia':[], 'agnews':[], 'trec':[]}\n",
    "    for dataset in worst_table:\n",
    "        table = tables1[n_shots][dataset]\n",
    "        for method in methods:\n",
    "            worst_table[dataset].append(f\"${table[method].mean():.3f}_{{\\\\pm{table[method].std():.3f}}}$\")\n",
    "\n",
    "    print(n_shots)\n",
    "    table = pd.DataFrame(worst_table, index=methods)\n",
    "    display(table)\n",
    "    print(table.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean(str_score, return_std=False):\n",
    "    score = str_score.split(\" (\")[0]\n",
    "    if score == \"NaN\":\n",
    "        mean, std = 0, 1\n",
    "    else:\n",
    "        mean, std = map(float, score.split(\" ± \"))\n",
    "    if return_std:\n",
    "        return mean, std\n",
    "    else:\n",
    "        return mean\n",
    "    \n",
    "    \n",
    "for n_shots in [0, 2]:\n",
    "    for dataset in ['sst2', 'dbpedia', 'agnews', 'trec']:\n",
    "        table = tables1[f'{n_shots}-shot'][d]\n",
    "        method_stats = {'direct': 0, 'channel': 0, 'calibrate': 0}\n",
    "        \n",
    "        for model, row in table.iterrows():\n",
    "            for method in ['direct', 'channel', 'calibrate']:\n",
    "                method_stats[method] += int(get_mean(row[f\"{method}_True\"]) > get_mean(row[f\"{method}_False\"]))\n",
    "        for method in method_stats:\n",
    "            method_stats[method] = round(method_stats[method] / len(table), 2)\n",
    "        \n",
    "        print(f\"{n_shots}-shot & {dataset} & {' & '.join([str(method_stats[k]) for k in ['direct', 'channel', 'calibrate']])}\\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min(model, dataset, n_shots, seed, method='direct_False'):\n",
    "    selected_runs = all_runs_df\n",
    "    for k, v in zip(['model', 'dataset', 'n_shots', 'seed', 'prediction_method', 'selection_method'], \n",
    "                    [model, dataset, n_shots, seed, method, 'random']):\n",
    "        selected_runs = selected_runs.loc[selected_runs[k] == v]\n",
    "    if len(selected_runs) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return min(selected_runs['scores'].values[0])\n",
    "\n",
    "for n_shots in [0, 2]:\n",
    "    seeds = [59] if n_shots == 0 else [59, 13, 21]\n",
    "    \n",
    "    for dataset in ['sst2', 'dbpedia', 'agnews', 'trec']:\n",
    "        method_stats = {k: 0 for k in ['direct_True', 'channel_True', 'channel_False', 'calibrate_True', 'calibrate_False']}\n",
    "        for model in table.index:\n",
    "            for seed in seeds:\n",
    "                baseline_score = get_min(model, dataset, n_shots, seed, 'direct_False')\n",
    "                for method in method_stats:\n",
    "                    method_score = get_min(model, dataset, n_shots, seed, method)\n",
    "                    if method_score > baseline_score:\n",
    "                        method_stats[method] += 1\n",
    "        for k in method_stats:\n",
    "            method_stats[k] = round(method_stats[k] / len(table.index) / len(seeds), 2)\n",
    "        print(f\"{n_shots}-shot & {dataset} & {' & '.join([str(v) for k, v in method_stats.items()])}\\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_table(table):\n",
    "    columns = table.columns.values\n",
    "    for model, row in table.iterrows():\n",
    "        res = f\"{model} & \"\n",
    "        best_mean, best_std, idx1, idx2 = 0, 1, 0, 0\n",
    "        for i, col in enumerate(columns):\n",
    "            score = row[col].split(\" (\")[0]\n",
    "            if score == \"NaN\":\n",
    "                mean, std = 0, 1\n",
    "            else:\n",
    "                mean, std = map(float, row[col].split(\" (\")[0].split(\" ± \"))\n",
    "            if mean > best_mean:\n",
    "                idx1 = i\n",
    "                best_mean = mean\n",
    "            if std < best_std:\n",
    "                idx2 = i\n",
    "                best_std = std\n",
    "        for i, col in enumerate(columns):\n",
    "            score = row[col].split(\" (\")[0]\n",
    "            if score == \"NaN\":\n",
    "                mean, std = 0, 1\n",
    "            else:\n",
    "                mean, std = map(float, row[col].split(\" (\")[0].split(\" ± \"))\n",
    "                \n",
    "            if i == idx1 and i == idx2:\n",
    "                row_line = \"$\\\\mathbf{{{:.3f}}}_{{\\\\underline{{\\\\pm{:.3f}}}}}$\"\n",
    "            elif i == idx1:\n",
    "                row_line = \"$\\\\mathbf{{{:.3f}}}_{{\\\\pm{:.3f}}}$\"\n",
    "            elif i == idx2:\n",
    "                row_line = \"${:.3f}_{{\\\\underline{{\\\\pm{:.3f}}}}}$\"\n",
    "            else:\n",
    "                row_line = \"${:.3f}_{{\\\\pm{:.3f}}}$\"\n",
    "            res += row_line.format(mean, std)\n",
    "            if i != len(columns) - 1:\n",
    "                res += '& '\n",
    "        print(f\"{res} \\\\\\\\\")\n",
    "for dataset in ['sst2', 'dbpedia', 'agnews', 'trec']:\n",
    "    print_table(tables1['0-shot'][dataset])\n",
    "    print('\\n\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3. Example selection methods\n",
    "\n",
    "* they are not resistant to the choice of template\n",
    "* they are not transferable from one model to another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_method = {\n",
    "    'sst2': {k: 'calibrate_True' if k in ['falcon-7b', 'falcon-40b', 'llama-65b'] else 'channel_True'\n",
    "            for k in model_specs},\n",
    "    'dbpedia': {k: 'channel_True' if k in ['gpt2-xl', 'bloom-1.7b', 'bloom-3b', 'bloom-7.1b', \n",
    "                                           'cerebras-13b', 'llama-7b'] else 'calibrate_True' \n",
    "                for k in model_specs},\n",
    "    'agnews': {k: 'calibrate_True' if k in ['opt-6.7b', 'falcon-1b', 'llama-13b', 'llama-30b', 'llama-65b'] \n",
    "              else 'channel_True' for k in model_specs},\n",
    "    'trec': {model: 'channel_True' for model in ['gpt2-xl', 'opt-1.3b', 'opt-30b', 'opt-66b', 'falcon-1b',\n",
    "                                                'pythia-6.9b', 'pythia-12b', 'cerebras-6.7b', 'cerebras-13b']},\n",
    "    }\n",
    "model_to_method['trec'].update({model: 'calibrate_True' \n",
    "                                for model in ['bloom-1.7b', 'llama-30b', 'llama-7b', 'llama-13b', \n",
    "                                              'llama-65b', 'falcon-40b', 'falcon-7b']})\n",
    "model_to_method['trec'].update({model: 'calibrate_False' for model in ['gptj', 'gpt-neox', \n",
    "                                                                       'bloom-7.1b', 'bloom-3b',\n",
    "                                                                      'opt-6.7b', 'gpt2-large']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_scores(dataset, model, n_shots, method, prediction_method='direct_False'):\n",
    "    selected_runs = all_runs_df.loc[all_runs_df['model'] == model]\n",
    "    selected_runs = selected_runs.loc[selected_runs['dataset'] == dataset]\n",
    "    selected_runs = selected_runs.loc[selected_runs['selection_method'] == method]\n",
    "    selected_runs = selected_runs.loc[selected_runs['prediction_method'] == prediction_method]\n",
    "    selected_runs = selected_runs.loc[selected_runs['n_shots'] == n_shots]\n",
    "    seeds = selected_runs['seed'].value_counts()\n",
    "    scores = []\n",
    "    if len(seeds) == 3:\n",
    "        pass\n",
    "    elif len(seeds) > 0:\n",
    "        print(f\"{dataset}_{model}_{method}_{n_shots} seeds: {seeds.keys()}\")\n",
    "        pass\n",
    "    else:\n",
    "        print(f\"!!!!!no seeds {dataset}_{model}_{method}_{n_shots} no seeds!!!!\")\n",
    "        pass\n",
    "    for seed, _ in seeds.items():\n",
    "        scores.extend(selected_runs.loc[selected_runs['seed'] == seed]['scores'].values[0])\n",
    "    return scores\n",
    "    \n",
    "tables_dataset = {}\n",
    "methods = ['random', 'implicitly_topic_models', 'z-ICL', 'CEIL'][:-1]\n",
    "n_shots_range = [2, 4, 8][:-1]\n",
    "for dataset in datasets:\n",
    "    print(dataset)\n",
    "    table = {f\"{m}-{k}\": {} for m in methods for k in n_shots_range}\n",
    "\n",
    "    for method in methods:\n",
    "        n_shot_res = {}\n",
    "        for n_shots in n_shots_range:\n",
    "            if method == 'z-ICL' and n_shots == 8:\n",
    "                continue\n",
    "            for model in names_to_checkpoints:\n",
    "                method_scores = get_scores(model=model, dataset=dataset, n_shots=n_shots, method=method)\n",
    "                if len(method_scores) > 0:\n",
    "                    mean = np.mean(method_scores)\n",
    "                    std = np.std(method_scores)\n",
    "                    res = f\"{mean:.3f} ± {std:.3f}\"\n",
    "                else:\n",
    "                    res = \"NaN\"\n",
    "                table[f\"{method}-{n_shots}\"][model] = res\n",
    "    table = pd.DataFrame(table, index=names_to_checkpoints.keys(), columns=['random-2', 'random-4',\n",
    "                                                                            'implicitly_topic_models-2', \n",
    "                                                                            'implicitly_topic_models-4', \n",
    "                                                                            'z-ICL-2', 'z-ICL-4'])\n",
    "    table.columns = ['random-2', 'random-4', 'ITM-2', 'ITM-4', 'z-ICL-2', 'z-ICL-4']\n",
    "    display(table)\n",
    "    tables_dataset[dataset] = table\n",
    "    print(f\"\\multirow{{3}}{{*}}{{{model}}} & 2 & {method_res['random'][2]} & {method_res['implicitly_topic_models'][2]} & {method_res['z-ICL'][2]} & {method_res['CEIL'][2]}\\\\\\\\\")\n",
    "    print(f\"& 4 &{method_res['random'][4]} & {method_res['implicitly_topic_models'][4]} & {method_res['z-ICL'][4]} & {method_res['CEIL'][4]}\\\\\\\\\")\n",
    "    print(f\"& 8 &{method_res['random'][8]} & {method_res['implicitly_topic_models'][8]} & {method_res['z-ICL'][8]} & {method_res['CEIL'][8]}\\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_dataset['sst2'].loc['gpt2-large']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(model, method, n_shots, dataset):\n",
    "    table = tables_dataset[dataset]\n",
    "    score = table.loc[model, f\"{method}-{n_shots}\"]\n",
    "    mean, std = map(float, score.split(\" ± \"))\n",
    "    res = f\"{mean:.2f}\\\\textsubscript{{{std:.2f}}}\"\n",
    "    return res\n",
    "    \n",
    "for model in ['gpt2-large', 'gpt2-xl', 'llama-7b', 'llama-13b', 'llama-30b', 'llama-65b']:\n",
    "    print(f\"\\\\multirow{{2}}{{*}}{{{model}}}\", end=\" & \")\n",
    "    for n_shots in [2, 4]:\n",
    "        if n_shots == 4:\n",
    "            print(f\"& {n_shots} &\", end=' ')\n",
    "        for dataset in datasets:\n",
    "            for method in ['random', 'ITM', 'z-ICL']:    \n",
    "                end = '' if (method == 'z-ICL') and (dataset == 'trec') else \" & \"\n",
    "                print(f\"{get_scores(model, method, n_shots, dataset)}\", end=end)\n",
    "        print('\\\\\\\\')\n",
    "    if model == 'gpt2-xl':\n",
    "        print('\\\\midrule')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_toptemplates(dataset, model, n_shots, method, prediction_method='direct_False', return_k=5):\n",
    "    selected_runs = all_runs_df.loc[all_runs_df['model'] == model]\n",
    "    selected_runs = selected_runs.loc[selected_runs['dataset'] == dataset]\n",
    "    selected_runs = selected_runs.loc[selected_runs['selection_method'] == method]\n",
    "    selected_runs = selected_runs.loc[selected_runs['prediction_method'] == prediction_method]\n",
    "    selected_runs = selected_runs.loc[selected_runs['n_shots'] == n_shots]\n",
    "    seeds = selected_runs['seed'].value_counts()\n",
    "    scores = []\n",
    "    if len(seeds) == 3:\n",
    "        for seed in [59, 13, 21]:\n",
    "            scores.extend(selected_runs.loc[selected_runs['seed'] == seed]['scores'].values[0])\n",
    "        all_templates = dataset_templates[dataset][59] + dataset_templates[dataset][13] + dataset_templates[dataset][21]\n",
    "    elif len(seeds) > 0:\n",
    "        print(f\"{dataset}_{model}_{method}_{n_shots} seeds: {seeds.keys()}\")\n",
    "        for seed in [59, 13]:\n",
    "            scores.extend(selected_runs.loc[selected_runs['seed'] == seed]['scores'].values[0])\n",
    "        all_templates = dataset_templates[dataset][59] + dataset_templates[dataset][13]\n",
    "    else:\n",
    "        print(f\"!!!!!no seeds {dataset}_{model}_{method}_{n_shots} no seeds!!!!\")\n",
    "        raise NotImplementedError\n",
    "    toptemplates = np.argsort(scores)[::-1]\n",
    "    res = []\n",
    "    for i in range(return_k):\n",
    "        res.append(all_templates[toptemplates[i]])\n",
    "    return res\n",
    "\n",
    "def calc_iou(list_a, list_b):\n",
    "    intersection, union = 0, len(list_b)\n",
    "    for template in list_a:\n",
    "        if template in list_b:\n",
    "            intersection += 1\n",
    "        else:\n",
    "            union += 1\n",
    "    return intersection / union\n",
    "\n",
    "pred_methods = [\"direct_False\", \"calibrate_True\", \"channel_True\"]\n",
    "\n",
    "heatmaps = {}\n",
    "models = list(names_to_checkpoints.keys())\n",
    "models = [x for x in models if not x.startswith(\"pythia\") and not x.startswith(\"cerebras\")]\n",
    "for return_k in [10]:\n",
    "    for n_shots in [2]:\n",
    "        for dataset in ['sst2', 'dbpedia', 'agnews', 'trec']:\n",
    "            heatmap = [[0 for _ in range(len(models))] for _ in range(len(models))]\n",
    "            method_toptemplates = {}\n",
    "            avg_iou = []\n",
    "            for i in range(len(models)):\n",
    "                for j in range(len(models)):\n",
    "                        method_i_toptemplates = get_toptemplates(model=models[i], dataset=dataset, \n",
    "                                                                 n_shots=n_shots, \n",
    "                                                                 method=\"random\", \n",
    "                                                                 return_k=return_k,\n",
    "                                                                 prediction_method=\"direct_False\")\n",
    "                        method_j_toptemplates = get_toptemplates(model=models[j], dataset=dataset, \n",
    "                                                                 n_shots=n_shots,\n",
    "                                                                 method=\"random\",\n",
    "                                                                 return_k=return_k,\n",
    "                                                                 prediction_method=\"direct_False\")\n",
    "\n",
    "                        heatmap[i][j] = calc_iou(method_i_toptemplates, method_j_toptemplates)\n",
    "                        \n",
    "            heatmaps[dataset] = heatmap\n",
    "            print(model, n_shots, heatmap[0][1], heatmap[0][2], heatmap[1][2], sep=\" & \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spelling = {'gpt2-large': 'GPT-2 Large',\n",
    " 'gpt2-xl': 'GPT-2 XL',\n",
    " 'gptj': 'GPT-J',\n",
    " 'gpt-neox': 'GPT-NeoX',\n",
    " 'opt-1.3b': 'OPT 1.3B',\n",
    " 'opt-6.7b': 'OPT 6.7B',\n",
    " 'opt-30b': 'OPT 30B',\n",
    " 'opt-66b': 'OPT 66B',\n",
    " 'bloom-1.7b': 'BLOOM 1.7B',\n",
    " 'bloom-3b': 'BLOOM 3B',\n",
    " 'bloom-7.1b': 'BLOOM 7.1B',\n",
    " 'pythia-6.9b': 'Pythia 6.9B',\n",
    " 'pythia-12b': 'Pythia 12B',\n",
    " 'cerebras-6.7b': 'Cerebras 6.7B',\n",
    " 'cerebras-13b': 'Cerebras 13B',\n",
    " 'llama-7b': 'LLaMA 7B',\n",
    " 'llama-13b': 'LLaMA 13B',\n",
    " 'llama-30b': 'LLaMA 30B',\n",
    " 'llama-65b': 'LLaMA 65B',\n",
    " 'falcon-1b': 'Falcon 1B',\n",
    " 'falcon-7b': 'Falcon 7B',\n",
    " 'falcon-40b': 'Falcon 40B'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "nice_fonts = {\n",
    "        # Use LaTeX to write all text\n",
    "        \"text.usetex\": True,\n",
    "        \"font.family\": \"Times New Roman\",\n",
    "        # Use 10pt font in plots, to match 10pt font in document\n",
    "        \"axes.labelsize\": 14,\n",
    "        \"font.size\": 14,\n",
    "        # Make the legend/label fonts a little smaller\n",
    "        \"legend.fontsize\": 14,\n",
    "        \"xtick.labelsize\": 12,\n",
    "        \"ytick.labelsize\": 12,\n",
    "}\n",
    "matplotlib.rcParams.update(nice_fonts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "dataset = \"trec\"\n",
    "sns.heatmap(\n",
    "    heatmaps[dataset],\n",
    "    cmap=sns.color_palette(\"coolwarm\", as_cmap=True),\n",
    "    yticklabels=[spelling[x] for x in models],\n",
    "    xticklabels=[spelling[x] for x in models],\n",
    ")\n",
    "plt.xticks(rotation=60, ha=\"right\")\n",
    "plt.savefig(f'../pictures/heatmap_{dataset}.pdf', format='pdf', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables3 = {}\n",
    "seeds = [59, 13, 21]\n",
    "conditions = [{'prediction_method': 'direct_False'}]\n",
    "for dataset in datasets:\n",
    "    table = []\n",
    "    conditions.append({'dataset': dataset})\n",
    "    for model in names_to_checkpoints:\n",
    "        conditions.append({'model': model})\n",
    "        entry = {'model': model}\n",
    "        for n_shots in [2, 4, 8]:\n",
    "            conditions.append({'n_shots': n_shots})\n",
    "            for method in ['random', 'implicitly_topic_models']:\n",
    "                conditions.append({'selection_method': method})\n",
    "                \n",
    "                res = aggregate_scores(all_runs_df, seeds, conditions)\n",
    "                entry.update({f\"{method}-{n_shots}\": res})\n",
    "                conditions.remove({'selection_method': method})\n",
    "            conditions.remove({'n_shots': n_shots})\n",
    "        conditions.remove({\"model\": model})\n",
    "        table.append(entry)\n",
    "    table = pd.DataFrame(table)\n",
    "    table.set_index('model', inplace=True)\n",
    "    tables3[dataset] = table\n",
    "    conditions.remove({'dataset': dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for d in datasets:\n",
    "    print(d)\n",
    "    display(tables3[d])\n",
    "print(\"-------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_runs_df[all_runs_df['selection_method'] == 'implicitly_topic_models']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_new",
   "language": "python",
   "name": "py39_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
